# # unified_scraper.py

# from abc import ABC, abstractmethod
# from typing import Optional, Any
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# import requests
# import time
# import os
# import logging
# from bs4 import BeautifulSoup
# from urllib.parse import urlparse, urljoin
# import hashlib
# from typing import Dict, List, Optional

# logger = logging.getLogger(__name__)

# class BaseScraper(ABC):
#     """Abstract base class for scrapers"""
    
#     @abstractmethod
#     def scrape_page(self, url: str) -> Optional[str]:
#         pass
    
#     @abstractmethod
#     def clean_content(self, content: str) -> str:
#         pass

# class StaticScraper(BaseScraper):
#     """For static content using requests"""
    
#     def __init__(self):
#         self.session = requests.Session()

#     def scrape_page(self, url: str) -> Optional[str]:
#         try:
#             headers = {
#                 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
#             }
#             response = self.session.get(url, headers=headers)
#             response.raise_for_status()
#             return response.text
#         except Exception as e:
#             logger.error(f"Static scraping failed: {e}")
#             return None

#     def clean_content(self, content: str) -> str:
#         """Clean the content (removes scripts, styles, etc.)"""
#         soup = BeautifulSoup(content, 'html.parser')
#         for element in soup(['script', 'style', 'iframe', 'noscript']):
#             element.decompose()
#         return soup.get_text(separator=' ', strip=True)

# class DynamicScraper(BaseScraper):
#     """For dynamic content using Selenium"""
    
#     def __init__(self, download_dir: str = "data"):
#         self.driver = None
#         self.download_dir = download_dir  # Set the download directory
    
#     def setup_driver(self):
#         """Sets up the Selenium WebDriver"""
#         options = Options()
#         options.add_argument("--headless")  # Run in headless mode, no UI
#         options.add_argument('--disable-gpu')  # Disable GPU acceleration
#         options.add_argument('--no-sandbox')  # Disable sandbox for Linux-based systems
#         options.add_argument('--disable-dev-shm-usage')  # Fixes issues with shared memory in Docker
#         self.driver = webdriver.Chrome(options=options)
#         self.driver.set_page_load_timeout(90)
    
#     def scrape_page(self, url: str) -> Optional[str]:
#         try:
#             if not self.driver:
#                 self.setup_driver()

#             self.driver.get(url)
#             page_source = self.driver.page_source
#             time.sleep(3)  # Allow time for dynamic content to load
#             return page_source
#         except Exception as e:
#             logger.error(f"Dynamic scraping failed: {e}")
#             return None
        
#     def create_site_folder(self, url: str) -> str:
#         """Create a folder name from URL"""
#         parsed_url = urlparse(url)
#         domain = parsed_url.netloc.replace('www.', '').split('.')[0]
#         folder_name = hashlib.md5(domain.encode()).hexdigest()[:8]
#         site_dir = os.path.join(self.download_dir, folder_name)
#         os.makedirs(site_dir, exist_ok=True)
#         return site_dir
    
#     def clean_content(self, content: str) -> str:
#         """Clean content scraped dynamically"""
#         soup = BeautifulSoup(content, 'html.parser')
#         for element in soup(['script', 'style', 'iframe', 'noscript']):
#             element.decompose()
#         return soup.get_text(separator=' ', strip=True)
    
#     def extract_images(self, html_content: str, base_url: str) -> List[Dict]:
#         """Extract image information from HTML"""
#         soup = BeautifulSoup(html_content, 'html.parser')
#         images = []

#         for img in soup.find_all('img'):
#             src = img.get('src')
#             if src:
#                 # Handle relative URLs
#                 if not src.startswith(('http://', 'https://')):
#                     src = urljoin(base_url, src)

#                 images.append({
#                     'url': src,
#                     'alt': img.get('alt', ''),
#                     'title': img.get('title', ''),
#                     'dimensions': {
#                         'width': img.get('width', ''),
#                         'height': img.get('height', '')
#                     }
#                 })

#         return images

# class UnifiedScraper:
#     """Factory class for choosing appropriate scraper based on the URL type"""
    
#     def __init__(self, download_dir: str = "data"):
#         self.static_scraper = StaticScraper()
#         self.dynamic_scraper = DynamicScraper(download_dir=download_dir)
        
#     def get_scraper(self, url: str) -> BaseScraper:
#         """Returns the appropriate scraper based on the URL"""
#         if self._needs_dynamic_scraping(url):
#             return self.dynamic_scraper
#         return self.static_scraper
    
#     def _needs_dynamic_scraping(self, url: str) -> bool:
#         """Detects if a page requires dynamic scraping (e.g., pages with JavaScript rendering)"""
#         dynamic_patterns = [
#             'angular.io',
#             'react.',
#             '#!',  # Hashbang URLs used in some JS-driven sites
#             'vue.',
#         ]
#         return any(pattern in url for pattern in dynamic_patterns)

#     def scrape(self, url: str) -> Optional[str]:
#         """Scrape content from the URL using the appropriate scraper"""
#         scraper = self.get_scraper(url)
#         raw_content = scraper.scrape_page(url)
#         if raw_content:
#             # Clean the raw content using the selected scraper's clean_content method
#             return scraper.clean_content(raw_content)
#         return None

#     def scrape_and_clean(self, url: str) -> Optional[str]:
#         """Unified method to scrape and clean content in one step"""
#         return self.scrape(url)

#     def create_site_folder(self, url: str) -> str:
#         """Create a folder using the dynamic scraper"""
#         return self.dynamic_scraper.create_site_folder(url)
    
#     def scrape_page(self, url: str) -> Optional[str]:
#         """Directly scrape the page using the appropriate scraper"""
#         scraper = self.get_scraper(url)
#         return scraper.scrape_page(url)
    
#     def clean_content(self, url: str, content: str) -> str:
#         """Clean the content using the appropriate scraper"""
#         scraper = self.get_scraper(url)
#         return scraper.clean_content(content)


#######################################################################################################

# unified_scraper.py
from abc import ABC, abstractmethod
from typing import Optional, Any, List, Dict
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import hashlib
import os
import time
import logging

logger = logging.getLogger(__name__)

class BaseScraper(ABC):
    """Abstract base class for scrapers"""

    @abstractmethod
    def scrape_page(self, url: str) -> Optional[str]:
        pass

    def clean_content(self, html_content: str) -> str:
        """Clean and extract meaningful content from HTML"""
        soup = BeautifulSoup(html_content, 'html.parser')
        for element in soup(['script', 'style', 'iframe', 'noscript']):
            element.decompose()
        text = soup.get_text(separator=' ', strip=True)
        return ' '.join(text.split())

class StaticScraper(BaseScraper):
    """For static content using requests"""

    def scrape_page(self, url: str) -> Optional[str]:
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"Static scraping failed: {e}")
            return None

class DynamicScraper(BaseScraper):
    """For dynamic content using Selenium"""

    def __init__(self):
        self.driver = None

    def setup_driver(self):
        options = Options()
        options.add_argument("--headless")
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        self.driver = webdriver.Chrome(options=options)
        self.driver.set_page_load_timeout(90)

    def scrape_page(self, url: str) -> Optional[str]:
        try:
            if not self.driver:
                self.setup_driver()
            self.driver.get(url)
            time.sleep(3)  # Allow time for dynamic content to load
            return self.driver.page_source
        except Exception as e:
            logger.error(f"Dynamic scraping failed: {e}")
            return None
        finally:
            if self.driver:
                self.driver.quit()

class UnifiedScraper:
    """Factory class to choose appropriate scraper"""

    def __init__(self):
        self.static_scraper = StaticScraper()
        self.dynamic_scraper = DynamicScraper()

    def _needs_dynamic_scraping(self, url: str) -> bool:
        """Detects if a page requires dynamic scraping"""
        dynamic_patterns = ['angular.io', 'react.', '#!', 'vue.']
        return any(pattern in url for pattern in dynamic_patterns)

    def scrape(self, url: str) -> Optional[str]:
        """Scrape and clean content from a URL using the appropriate scraper"""
        scraper = self.dynamic_scraper if self._needs_dynamic_scraping(url) else self.static_scraper
        raw_content = scraper.scrape_page(url)
        if raw_content:
            return scraper.clean_content(raw_content)
        return None

    
    def scrape_website(self, url: str) -> Optional[str]:
        """Scrape content from the given URL."""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logging.error(f"Error scraping website: {str(e)}")
            return None
    
    def clean_content(self, html_content: str) -> str:
        """Clean HTML content."""
        soup = BeautifulSoup(html_content, 'html.parser')
        for element in soup(['script', 'style', 'iframe', 'noscript']):
            element.decompose()
        text = soup.get_text(separator=' ', strip=True)
        return ' '.join(text.split())
