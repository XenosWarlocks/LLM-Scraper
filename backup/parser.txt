

################################################################################################
        # Gemini prompt template
        # self.prompt = ChatPromptTemplate.from_template(
        #     "Search through this content and find what's asked. If you find something relevant, "
        #     "return it exactly as found. If nothing matches, return 'NO_MATCH'.\n\n"
        #     "Content: {dom_content}\n\n"
        #     "Find this: {parse_description}\n\n"
        #     "Guidelines:\n"
        #     "1. If searching for a model number and you find an exact match, return just that\n"
        #     "2. If searching for a product title with model, return the complete relevant phrase\n"
        #     "3. Be flexible - if something seems to match what's being asked for, include it\n"
        #     "4. Don't explain or add commentary - just return what you find\n"
        #     "5. Return 'NO_MATCH' only if you can't find anything relevant"
        # )
################################################################################################

############################################################################################################

self.prompt = ChatPromptTemplate.from_template(
            '''
            You are a website information retrieval agent. Your task is to analyze the provided website content and answer the user's query accurately. If the information is present, return it exactly as found on the site. If the website content does not contain the answer, return only 'NO_MATCH'.

            Website Content: {dom_content}

            Query: {parse_description}

            Instructions:

            1. **Precise Retrieval**: If the query targets specific data (e.g., model number, serial number, file name), return only the exact match if found. 
            2. **Contextual Answers**: For broader queries (e.g., product description, specifications, image details), return the smallest, self-contained excerpt from the website content that fully answers the query. Include relevant surrounding context if it adds clarity.
            3. **Multimedia and Files**: If the query relates to images or downloadable files (e.g., PDFs, manuals), provide the image URL or file link exactly as presented on the website. If the query asks about the *content* of a file, extract and return the relevant information if accessible; otherwise, indicate "File content not accessible."
            4. **Handling Variations**: Be flexible - if an exact match isn't found but something closely matches the query, return the closest relevant information. Prioritize informative excerpts over shorter, ambiguous ones while staying focused on the query.
            5. **No Interpretation**: Return only the extracted information without rephrasing, summarizing, or adding any commentary.
            6. **Strict NO_MATCH**: Return 'NO_MATCH' only if the website content does not contain any relevant information related to the query.
            
            Examples:
            
            Example 1:
            Content: "<h1>Product X</h1><p>Serial: AB123</p><img src='image.jpg' alt='Product Image'><a href='manual.pdf'>User Manual</a>"
            Query: "Serial number"
            Response: "AB123"

            Example 2:
            Content: "...<p>Specifications: Weight: 10kg, Dimensions: 20x30cm</p>..."
            Query: "What are the dimensions?"
            Response: "Dimensions: 20x30cm"

            Example 3:
            Content: "...<img src='image.jpg' alt='Product Image'>..."
            Query: "Image of the product"
            Response: "'image.jpg'"

            Example 4:
            Content: "...<a href='manual.pdf'>User Manual</a>..."
            Query: "Where can I find the user manual?"
            Response: "'manual.pdf'"

            Your Response:
            '''
        )

        def parse_with_gemini(self, dom_chunks: List[str], parse_description: str) -> str:
        """Parse content chunks using Gemini"""
        chain = self.prompt | self.model
        found_results = []
        
        chunk_size = 3
        for i in range(0, len(dom_chunks), chunk_size):
            chunk_group = " ".join(dom_chunks[i:i + chunk_size])
            try:
                response = chain.invoke({
                    "dom_content": chunk_group,
                    "parse_description": parse_description
                })
                content = response.content if hasattr(response, 'content') else str(response)
                content = content.strip()
                
                if content and content != 'NO_MATCH':
                    found_results.append(content)
                
            except Exception as e:
                logger.error(f"Error processing chunk group {i}: {str(e)}")
                continue
        
        return "\n".join(dict.fromkeys(found_results)) if found_results else "NO_MATCH"

        # batch
    async def parse_with_gemini_async(self, dom_chunks: List[str], parse_description: str) -> str:
        """Asynchronous version of parse_with_gemini"""
        found_results = []
        chunk_size = 3
        for i in range(0, len(dom_chunks), chunk_size):
            chunk_group = " ".join(dom_chunks[i:i + chunk_size])
            try:
                response = await self.model.agenerate(
                    messages=[{
                        "role": "user",
                        "content": self.prompt.format(
                            dom_content=chunk_group,
                            parse_description=parse_description
                        )
                    }]
                )
                content = response.generations[0].text.strip()
                if content and content != 'NO_MATCH':
                    found_results.append(content)
            except Exception as e:
                logger.error(f"Error processing chunk group {i}: {str(e)}")
                continue
        return "\n".join(dict.fromkeys(found_results)) if found_results else "NO_MATCH"

##############################################################################################################

# parse.py
import json
from urllib.parse import urlparse
from langchain_google_genai import ChatGoogleGenerativeAI
import google.generativeai as genai
from langchain_core.prompts import ChatPromptTemplate
from typing import Optional, List, Dict
import logging
import os
from dataclasses import dataclass
from datetime import datetime
from bs4 import BeautifulSoup


from utils.api_limiting import rate_limit
from utils.parse_config import ParserConfig
from utils.url_validator import URLValidator

from content_analyzer import ContentAnalyzer, ImageMatch
from unified_scraper import UnifiedScraper
from site_scraper import SiteScraper
from loader import ImageLoader

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ParseResult:
    """Data class to hold parsing results"""
    site_id: str
    content_analysis: Dict
    image_matches: List[ImageMatch]
    raw_content: str
    gemini_parse_result: Optional[str] = None
    downloaded_files: List[str] = None
    pdf_links: List[str] = None
    timestamp: str = datetime.now().isoformat()

class UnifiedParser:
    def __init__(self, config: ParserConfig):
        """Initialize the UnifiedParser with configurations loaded from YAML and .env"""
        # Initialize Gemini
        genai.configure(api_key=config.api_key)
        self.model = ChatGoogleGenerativeAI(model=config.model_name)
        
        # Initialize other components
        self.content_analyzer = ContentAnalyzer(api_key=config.api_key, data_dir=config.data_dir)
        self.site_scraper = SiteScraper(download_dir=config.data_dir)
        self.image_loader = ImageLoader()
        
        self.data_dir = config.data_dir
        self.results_dir = os.path.join(config.data_dir, "parse_results")
        os.makedirs(self.results_dir, exist_ok=True)
        

        self.prompt = ChatPromptTemplate.from_template(
            '''
            You are a website information retrieval agent. Your task is to analyze the provided website content and answer the user's query accurately. If the information is present, return it exactly as found on the site. If the website content does not contain the answer, return only 'NO_MATCH'.

            Website Content: {dom_content}

            Query: {parse_description}

            Instructions:

            1. **Precise Retrieval**: If the query targets specific data (e.g., model number, serial number, file name), return only the exact match if found. 
            2. **Contextual Answers**: For broader queries (e.g., product description, specifications, image details), return the smallest, self-contained excerpt from the website content that fully answers the query. Include relevant surrounding context if it adds clarity.
            3. **Multimedia and Files**: If the query relates to images or downloadable files (e.g., PDFs, manuals), provide the image URL or file link exactly as presented on the website. If the query asks about the *content* of a file, extract and return the relevant information if accessible; otherwise, indicate "File content not accessible."
            4. **Handling Variations**: Be flexible - if an exact match isn't found but something closely matches the query, return the closest relevant information. Prioritize informative excerpts over shorter, ambiguous ones while staying focused on the query.
            5. **No Interpretation**: Return only the extracted information without rephrasing, summarizing, or adding any commentary.
            6. **Strict NO_MATCH**: Return 'NO_MATCH' only if the website content does not contain any relevant information related to the query.
            
            Examples:
            
            Example 1:
            Content: "<h1>Product X</h1><p>Serial: AB123</p><img src='image.jpg' alt='Product Image'><a href='manual.pdf'>User Manual</a>"
            Query: "Serial number"
            Response: "AB123"

            Example 2:
            Content: "...<p>Specifications: Weight: 10kg, Dimensions: 20x30cm</p>..."
            Query: "What are the dimensions?"
            Response: "Dimensions: 20x30cm"

            Example 3:
            Content: "...<img src='image.jpg' alt='Product Image'>..."
            Query: "Image of the product"
            Response: "'image.jpg'"

            Example 4:
            Content: "...<a href='manual.pdf'>User Manual</a>..."
            Query: "Where can I find the user manual?"
            Response: "'manual.pdf'"

            Your Response:
            '''
        )


    def parse_website(
            self,
            url: str,
            min_confidence: float = 0.7,
            show_all_images: bool = False,
            parse_description: Optional[str] = None,
            **kwargs
        ) -> ParseResult:
        """Parse a website using both Gemini and ContentAnalyzer capabilities"""
        
        try:
            
            # Validate and normalize the URL
            if not URLValidator.is_valid_url(url):
                raise ValueError(f"Invalid URL: {url}")
            
            normalized_url = URLValidator.normalize_url(url)
            # Create site-specific folder
            site_dir = self.site_scraper.create_site_folder(url)
            site_id = os.path.basename(site_dir)
            
            # Step 2: Scrape the webpage using UnifiedScraper
            logger.info(f"Scraping website: {url}")
            unified_scraper = UnifiedScraper()
            html_content = unified_scraper.scrape_website(url)
            if not html_content:
                raise Exception("Failed to scrape website")
            
            # Step 3: Clean the content using UnifiedScraper's cleaning method
            logger.info("Cleaning content using UnifiedScraper")
            cleaned_content = unified_scraper.clean_content(html_content)
            
            # Extract images
            logger.info("Extracting images from website")
            images = self.site_scraper.extract_images(html_content, url)
            image_urls = [
                URLValidator.resolve_relative_url(normalized_url, img['url']) for img in images
]
            
            # Download images
            downloaded_images = self.image_loader.download_images_from_html(html_content, normalized_url)
            downloaded_files = [path[1] for path in downloaded_images] if downloaded_images else []
            
            # Store the base URL for relative link resolution
            self.site_scraper.base_url = url
        
            # Find document links (PDF/DOCX)
            logger.info("Finding PDF and DOCX documents")
            document_links = self.find_pdf_links(html_content)
            logger.info(f"Found {len(document_links)} documents: {document_links}")
            
            # Analyze content
            logger.info("Analyzing website content")
            content_analysis = self.content_analyzer.analyze_content(cleaned_content)
            
            # Find matching images based on the show_all_images flag
            logger.info("Finding matching images")
            image_matches = self.content_analyzer.find_matching_images(
                content_analysis=content_analysis,
                available_images=image_urls,
                threshold=(0.0 if show_all_images else min_confidence)
            )
            
            # Perform Gemini parsing if description provided
            gemini_result = None
            if parse_description:
                processed_chunks = self.preprocess_content(cleaned_content)
                gemini_result = self.parse_with_gemini(processed_chunks, parse_description)
            
            # Create parse result
            result = ParseResult(
                site_id=site_id,
                content_analysis=content_analysis,
                image_matches=image_matches,
                raw_content=cleaned_content,
                gemini_parse_result=gemini_result,
                downloaded_files=downloaded_files,
                pdf_links=document_links  # Store the document links
            )
            
            # Save results
            self._save_parse_result(result)
            
            return result
            
        except Exception as e:
            logger.error(f"Error parsing website: {str(e)}")
            raise

    @rate_limit(calls=10, period=60)  # 10 calls per minute
    def parse_with_gemini(self, dom_chunks: List[str], parse_description: str) -> str:
        """Parse content chunks using Gemini"""
        chain = self.prompt | self.model
        found_results = []
        
        chunk_size = 3
        for i in range(0, len(dom_chunks), chunk_size):
            chunk_group = " ".join(dom_chunks[i:i + chunk_size])
            try:
                response = chain.invoke({
                    "dom_content": chunk_group,
                    "parse_description": parse_description
                })
                content = response.content if hasattr(response, 'content') else str(response)
                content = content.strip()
                
                if content and content != 'NO_MATCH':
                    found_results.append(content)
                
            except Exception as e:
                logger.error(f"Error processing chunk group {i}: {str(e)}")
                continue
        
        return "\n".join(dict.fromkeys(found_results)) if found_results else "NO_MATCH"

    def preprocess_content(self, html_content: str) -> List[str]:
        """Preprocess HTML content"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        for element in soup(['script', 'style']):
            element.decompose()
        
        texts = []
        for element in soup.stripped_strings:
            text = element.strip()
            if text:
                texts.append(text)
        
        return texts

    def find_pdf_links(self, html_content: str) -> List[str]:
        """Find PDF and DOCX links in HTML content"""
        soup = BeautifulSoup(html_content, 'html.parser')
        document_links = []
        
        # Define allowed document extensions
        ALLOWED_EXTENSIONS = ('.pdf', '.docx')
        
        for link in soup.find_all('a'):
            href = link.get('href')
            if href:
                href = href.strip()
                
                # Check if the link ends with allowed extensions
                if href.lower().endswith(ALLOWED_EXTENSIONS):
                    # Make absolute URL if relative
                    if not href.startswith(('http://', 'https://')):
                        try:
                            base_url = urlparse(self.site_scraper.base_url)
                            if href.startswith('/'):
                                href = f"{base_url.scheme}://{base_url.netloc}{href}"
                            else:
                                href = f"{base_url.scheme}://{base_url.netloc}/{base_url.path.rstrip('/')}/{href}"
                        except (AttributeError, ValueError):
                            continue
                    
                    document_links.append(href)
        
        # Remove duplicates while preserving order
        return list(dict.fromkeys(document_links))

    def update_image_verification(self, site_id: str, image_url: str, verified: bool) -> bool:
        """Update user verification for an image match"""
        try:
            result_path = os.path.join(self.results_dir, f"{site_id}.json")
            if not os.path.exists(result_path):
                logger.error(f"No parse result found for site ID: {site_id}")
                return False
            
            with open(result_path, 'r') as f:
                data = json.load(f)
            
            for img_match in data['image_matches']:
                if img_match['url'] == image_url:
                    image_match = ImageMatch(
                        url=img_match['url'],
                        path=img_match['path'],
                        confidence=img_match['confidence'],
                        category=img_match['category'],
                        tags=img_match['tags']
                    )
                    
                    img_match['user_verified'] = verified
                    img_match['verification_timestamp'] = datetime.now().isoformat()
                    
                    self.content_analyzer.update_user_choice(image_match, verified)
                    
                    with open(result_path, 'w') as f:
                        json.dump(data, f, indent=2)
                    
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error updating image verification: {str(e)}")
            return False

    def _save_parse_result(self, result: ParseResult):
        """Save parse result to file"""
        result_path = os.path.join(self.results_dir, f"{result.site_id}.json")
        with open(result_path, 'w') as f:
            json.dump(result.__dict__, f, indent=2)
################################################################################################

'''
            You are a website information retrieval agent. Your task is to analyze the provided website content and answer the user's query accurately. If the information is present, return it exactly as found on the site. If the website content does not contain the answer, return only 'NO_MATCH'.

            Website Content: {dom_content}

            Query: {parse_description}

            Instructions:

            1. **Precise Retrieval**: If the query targets specific data (e.g., model number, serial number, file name), return only the exact match if found. 
            2. **Contextual Answers**: For broader queries (e.g., product description, specifications, image details), return the smallest, self-contained excerpt from the website content that fully answers the query. Include relevant surrounding context if it adds clarity.
            3. **Multimedia and Files**: If the query relates to images or downloadable files (e.g., PDFs, manuals), provide the image URL or file link exactly as presented on the website. If the query asks about the *content* of a file, extract and return the relevant information if accessible; otherwise, indicate "File content not accessible."
            4. **Handling Variations**: Be flexible - if an exact match isn't found but something closely matches the query, return the closest relevant information. Prioritize informative excerpts over shorter, ambiguous ones while staying focused on the query.
            5. **No Interpretation**: Return only the extracted information without rephrasing, summarizing, or adding any commentary.
            6. **Strict NO_MATCH**: Return 'NO_MATCH' only if the website content does not contain any relevant information related to the query.
            
            Examples:
            
            Example 1:
            Content: "<h1>Product X</h1><p>Serial: AB123</p><img src='image.jpg' alt='Product Image'><a href='manual.pdf'>User Manual</a>"
            Query: "Serial number"
            Response: "AB123"

            Example 2:
            Content: "...<p>Specifications: Weight: 10kg, Dimensions: 20x30cm</p>..."
            Query: "What are the dimensions?"
            Response: "Dimensions: 20x30cm"

            Example 3:
            Content: "...<img src='image.jpg' alt='Product Image'>..."
            Query: "Image of the product"
            Response: "'image.jpg'"

            Example 4:
            Content: "...<a href='manual.pdf'>User Manual</a>..."
            Query: "Where can I find the user manual?"
            Response: "'manual.pdf'"

            Your Response:
            '''

################################################################################################


# parse.py
import json
from urllib.parse import urlparse
from langchain_google_genai import ChatGoogleGenerativeAI
import google.generativeai as genai
from langchain_core.prompts import ChatPromptTemplate
from typing import Optional, List, Dict
import logging
import os
from dataclasses import dataclass
from datetime import datetime
from bs4 import BeautifulSoup
import aiohttp
import asyncio

from utils.api_limiting import rate_limit
from utils.parse_config import ParserConfig
from utils.url_validator import URLValidator

from content_analyzer import ContentAnalyzer, ImageMatch
from unified_scraper import UnifiedScraper
from site_scraper import SiteScraper
from loader import ImageLoader
from batch_processor import BatchURLProcessor, BatchProcessingResult
from doc_downloader import DocumentDownloader


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ParseResult:
    """Data class to hold parsing results"""
    site_id: str
    content_analysis: Dict
    image_matches: List[ImageMatch]
    raw_content: str
    gemini_parse_result: Optional[str] = None
    downloaded_files: List[str] = None
    pdf_links: List[str] = None
    timestamp: str = datetime.now().isoformat()

class UnifiedParser:
    def __init__(self, config: ParserConfig):
        """Initialize the UnifiedParser with configurations loaded from YAML and .env"""
        # Initialize Gemini
        genai.configure(api_key=config.api_key)
        self.model = ChatGoogleGenerativeAI(model=config.model_name)
        
        # Initialize other components
        self.content_analyzer = ContentAnalyzer(api_key=config.api_key, data_dir=config.data_dir)
        self.site_scraper = SiteScraper(download_dir=config.data_dir)
        self.image_loader = ImageLoader()
        
        self.data_dir = config.data_dir
        self.results_dir = os.path.join(config.data_dir, "parse_results")
        os.makedirs(self.results_dir, exist_ok=True)
        
        self.doc_downloader = DocumentDownloader(
        base_url="",  # Will be set during parsing
        download_dir=config.data_dir
        )
        

        self.prompt = ChatPromptTemplate.from_template(
            
        )
    def parse_website(
            self,
            url: str,
            min_confidence: float = 0.7,
            show_all_images: bool = False,
            parse_description: Optional[str] = None,
            **kwargs
        ) -> ParseResult:
        try:
            if not URLValidator.is_valid_url(url):
                raise ValueError(f"Invalid URL: {url}")
            normalized_url = URLValidator.normalize_url(url)
            site_dir = self.site_scraper.create_site_folder(url)
            site_id = os.path.basename(site_dir)
            logger.info(f"Scraping website: {url}")
            unified_scraper = UnifiedScraper()
            html_content = unified_scraper.scrape_website(url)
            if not html_content:
                raise Exception("Failed to scrape website")
            logger.info("Cleaning content using UnifiedScraper")
            cleaned_content = unified_scraper.clean_content(html_content)
            logger.info("Extracting images from website")
            images = self.site_scraper.extract_images(html_content, url)
            image_urls = [
                URLValidator.resolve_relative_url(normalized_url, img['url']) for img in images
]
            downloaded_images = self.image_loader.download_images_from_html(html_content, normalized_url)
            downloaded_files = [path[1] for path in downloaded_images] if downloaded_images else []
            self.site_scraper.base_url = url
            logger.info("Finding PDF and DOCX documents")
            document_links = self.find_pdf_links(html_content)
            logger.info(f"Found {len(document_links)} documents: {document_links}")
            logger.info("Analyzing website content")
            content_analysis = self.content_analyzer.analyze_content(cleaned_content)
            logger.info("Finding matching images")
            image_matches = self.content_analyzer.find_matching_images(
                content_analysis=content_analysis,
                available_images=image_urls,
                threshold=(0.0 if show_all_images else min_confidence)
            )
            gemini_result = None
            if parse_description:
                processed_chunks = self.preprocess_content(cleaned_content)
                gemini_result = self.parse_with_gemini(processed_chunks, parse_description)
            result = ParseResult(
                site_id=site_id,
                content_analysis=content_analysis,
                image_matches=image_matches,
                raw_content=cleaned_content,
                gemini_parse_result=gemini_result,
                downloaded_files=downloaded_files,
                pdf_links=document_links  # Store the document links
            )
            self._save_parse_result(result)
            return result
        except Exception as e:
            logger.error(f"Error parsing website: {str(e)}")
            raise
    @rate_limit(calls=10, period=60)  # 10 calls per minute
    def parse_with_gemini(self, dom_chunks: List[str], parse_description: str) -> str:
        chain = self.prompt | self.model
        found_results = []
        chunk_size = 3
        for i in range(0, len(dom_chunks), chunk_size):
            chunk_group = " ".join(dom_chunks[i:i + chunk_size])
            try:
                response = chain.invoke({
                    "dom_content": chunk_group,
                    "parse_description": parse_description
                })
                content = response.content if hasattr(response, 'content') else str(response)
                content = content.strip()
                
                if content and content != 'NO_MATCH':
                    found_results.append(content)
            except Exception as e:
                logger.error(f"Error processing chunk group {i}: {str(e)}")
                continue
        return "\n".join(dict.fromkeys(found_results)) if found_results else "NO_MATCH"
    async def download_documents(self, doc_links: List[str], site_id: str) -> Dict[str, List[str]]:
        try:
            doc_dir = os.path.join(self.data_dir, site_id, "documents")
            os.makedirs(doc_dir, exist_ok=True)
            self.doc_downloader.base_url = self.site_scraper.base_url
            self.doc_downloader.download_dir = doc_dir
            
            async with aiohttp.ClientSession() as session:
                downloaded_files = await self.doc_downloader.download_documents_async(
                    doc_links=doc_links,
                    session=session
                )
            return downloaded_files
        except Exception as e:
            logger.error(f"Error downloading documents: {str(e)}")
            return {}
    async def parse_with_gemini_async(self, dom_chunks: List[str], parse_description: str) -> str:
        found_results = []
        chunk_size = 3
        for i in range(0, len(dom_chunks), chunk_size):
            chunk_group = " ".join(dom_chunks[i:i + chunk_size])
            try:
                response = await self.model.agenerate(
                    messages=[{
                        "role": "user",
                        "content": self.prompt.format(
                            dom_content=chunk_group,
                            parse_description=parse_description
                        )
                    }]
                )
                content = response.generations[0].text.strip()
                if content and content != 'NO_MATCH':
                    found_results.append(content)
            except Exception as e:
                logger.error(f"Error processing chunk group {i}: {str(e)}")
                continue
        return "\n".join(dict.fromkeys(found_results)) if found_results else "NO_MATCH"
    def create_batch_processor(self, max_concurrent: int = 5, timeout: int = 30) -> BatchURLProcessor:
        return BatchURLProcessor(
            scraper=self.site_scraper,
            parser=self,
            doc_downloader=self.doc_downloader,
            max_concurrent=max_concurrent,
            timeout=timeout
        )
    def preprocess_content(self, html_content: str) -> List[str]:
        soup = BeautifulSoup(html_content, 'html.parser')
        for element in soup(['script', 'style']):
            element.decompose()
        texts = []
        for element in soup.stripped_strings:
            text = element.strip()
            if text:
                texts.append(text)
        return texts
    def find_pdf_links(self, html_content: str) -> List[str]:
        soup = BeautifulSoup(html_content, 'html.parser')
        document_links = []
        ALLOWED_EXTENSIONS = ('.pdf', '.docx')
        for link in soup.find_all('a'):
            href = link.get('href')
            if href:
                href = href.strip()
                if href.lower().endswith(ALLOWED_EXTENSIONS):
                    if not href.startswith(('http://', 'https://')):
                        try:
                            base_url = urlparse(self.site_scraper.base_url)
                            if href.startswith('/'):
                                href = f"{base_url.scheme}://{base_url.netloc}{href}"
                            else:
                                href = f"{base_url.scheme}://{base_url.netloc}/{base_url.path.rstrip('/')}/{href}"
                        except (AttributeError, ValueError):
                            continue
                    document_links.append(href)
        return list(dict.fromkeys(document_links))
    def update_image_verification(self, site_id: str, image_url: str, verified: bool) -> bool:
        try:
            result_path = os.path.join(self.results_dir, f"{site_id}.json")
            if not os.path.exists(result_path):
                logger.error(f"No parse result found for site ID: {site_id}")
                return False
            with open(result_path, 'r') as f:
                data = json.load(f)
            for img_match in data['image_matches']:
                if img_match['url'] == image_url:
                    image_match = ImageMatch(
                        url=img_match['url'],
                        path=img_match['path'],
                        confidence=img_match['confidence'],
                        category=img_match['category'],
                        tags=img_match['tags']
                    )
                    img_match['user_verified'] = verified
                    img_match['verification_timestamp'] = datetime.now().isoformat()
                    self.content_analyzer.update_user_choice(image_match, verified)
                    with open(result_path, 'w') as f:
                        json.dump(data, f, indent=2)
                    return True
            return Fals
        except Exception as e:
            logger.error(f"Error updating image verification: {str(e)}")
            return False
    def _save_parse_result(self, result: ParseResult):
        result_path = os.path.join(self.results_dir, f"{result.site_id}.json")
        with open(result_path, 'w') as f:
            json.dump(result.__dict__, f, indent=2)